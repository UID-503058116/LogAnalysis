import sys
import json
import asyncio
import argparse
from pathlib import Path
from logginganalysis import LogAnalyzer
from logginganalysis.chunking import LogChunker
from logginganalysis.utils.logging_config import setup_logging


def progress_callback(update):
    """è¿›åº¦å›è°ƒå‡½æ•°ã€‚"""
    step = update.get("step", "unknown")
    message = update.get("message", "")
    progress = update.get("progress", "")

    if step == "extraction" and "chunk_index" in update:
        chunk_idx = update.get("chunk_index", 0)
        total = update.get("total_chunks", 0)
        status = update.get("status", "processing")

        if status == "completed":
            exceptions = update.get("exceptions_found", 0)
            behaviors = update.get("behaviors_found", 0)
            libraries = update.get("libraries_found", 0)
            print(
                f"\r  [{chunk_idx}/{total}] {progress} - "
                f"å‘ç°: {exceptions} å¼‚å¸¸, {behaviors} è¡Œä¸º, {libraries} åº“",
                end="",
                flush=True,
            )
        elif status == "failed":
            error = update.get("error", "æœªçŸ¥é”™è¯¯")
            print(f"\n  âŒ [{chunk_idx}/{total}] å¤„ç†å¤±è´¥: {error}")
        else:
            print(f"\r  [{chunk_idx}/{total}] {progress} - å¤„ç†ä¸­...", end="", flush=True)
    elif progress:
        print(f"\n  â–¶ {step.upper()}: {message} ({progress})")
    else:
        print(f"\n  â–¶ {step.upper()}: {message}")


async def main():
    print("=" * 60)
    print("æ—¥å¿—åˆ†ææ¼”ç¤º - LoggingAnalysis Demo")
    print("=" * 60)

    # é…ç½®æ—¥å¿—ç³»ç»Ÿ
    print("\n[é…ç½®] åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ...")
    setup_logging(level="INFO")
    print("[é…ç½®] æ—¥å¿—ç³»ç»Ÿå·²å°±ç»ª (çº§åˆ«: INFO)")

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument("--log-file", type=Path, help="æ—¥å¿—æ–‡ä»¶è·¯å¾„", required=True)
    parser.add_argument("--output-file", type=Path, help="æŠ¥å‘Šæ–‡ä»¶è·¯å¾„", required=True)
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=4000,
        help="æ¯ä¸ªchunkçš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤: 4000ï¼‰",
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=200,
        help="ç›¸é‚»chunkä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼ˆé»˜è®¤: 200ï¼‰",
    )
    args = parser.parse_args()
    assert Path(args.log_file).exists(), f"æ—¥å¿—æ–‡ä»¶ {args.log_file} ä¸å­˜åœ¨"
    assert not Path(args.output_file).exists(), f"æŠ¥å‘Šæ–‡ä»¶ {args.output_file} å·²å­˜åœ¨"

    # è¿›åº¦è·Ÿè¸ª
    progress_updates = []

    def wrapped_callback(update):
        progress_updates.append(update)
        progress_callback(update)

    # åˆ›å»ºåˆ†æå™¨
    print("\n[åˆå§‹åŒ–] åˆ›å»ºæ—¥å¿—åˆ†æå™¨...")
    print(f"[åˆå§‹åŒ–] Chunkå¤§å°: {args.chunk_size} å­—ç¬¦, é‡å : {args.chunk_overlap} å­—ç¬¦")

    # åˆ›å»ºè‡ªå®šä¹‰chunker
    chunker = LogChunker(
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
    )

    analyzer = LogAnalyzer(chunker=chunker, progress_callback=wrapped_callback)
    print("[åˆå§‹åŒ–] åˆ†æå™¨å·²å°±ç»ª\n")

    # è¯»å–æ—¥å¿—æ–‡ä»¶
    log_file = str(args.log_file)
    print(f"[è¯»å–] åŠ è½½æ—¥å¿—æ–‡ä»¶: {log_file}")
    try:
        with open(log_file, "r") as f:
            log_content = f.read()
        print(f"[è¯»å–] æˆåŠŸè¯»å– {len(log_content)} å­—èŠ‚\n")
    except FileNotFoundError:
        print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°æ—¥å¿—æ–‡ä»¶ {log_file}")
        print("è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨åå†è¿è¡Œæ­¤æ¼”ç¤ºè„šæœ¬ã€‚")
        sys.exit(1)

    # å¼€å§‹åˆ†æ
    print("=" * 60)
    print("å¼€å§‹åˆ†æ")
    print("=" * 60)

    try:
        report = await analyzer.analyze(
            log_content=log_content, log_source=log_file, enable_search=False
        )
    except Exception as e:
        print(f"\n\nâŒ åˆ†æå¤±è´¥: {e}", file=sys.stderr)
        raise

    print("\n" + "=" * 60)
    print("åˆ†æå®Œæˆ!")
    print("=" * 60)

    # æ˜¾ç¤ºè¿›åº¦ç»Ÿè®¡
    print(f"\nğŸ“Š è¿›åº¦ç»Ÿè®¡:")
    print(f"  æ€»è¿›åº¦æ›´æ–°æ¬¡æ•°: {len(progress_updates)}")

    steps = {}
    for update in progress_updates:
        step = update.get("step", "unknown")
        steps[step] = steps.get(step, 0) + 1

    print(f"  å„æ­¥éª¤æ›´æ–°æ¬¡æ•°:")
    for step, count in sorted(steps.items()):
        print(f"    - {step}: {count}")

    # æ˜¾ç¤ºåˆ†æç»“æœ
    print("\n" + "=" * 60)
    print("åˆ†æç»“æœ (Analysis Results)")
    print("=" * 60)

    print(f"\nğŸ“ æ•´ä½“æ‘˜è¦ (Overall Summary)")
    print("-" * 60)
    print(report.analysis.overall_summary)
    print()

    if report.analysis.error_chain:
        print(f"\nğŸ”— é”™è¯¯é“¾ (Error Chain)")
        print("-" * 60)
        print(f"æ ¹æœ¬åŸå› : {report.analysis.error_chain.root_cause}")
        print()
        print("é”™è¯¯ä¼ æ’­é“¾:")
        for step in report.analysis.error_chain.chain:
            print(f"  æ­¥éª¤ {step.get('step')}:")
            print(f"    äº‹ä»¶: {step.get('event')}")
            print(f"    å½±å“: {step.get('impact')}")
        print()
        print(f"æœ€ç»ˆç»“æœ: {report.analysis.error_chain.final_outcome}")
        print()

    if report.analysis.key_findings:
        print(f"\nğŸ’¡ å…³é”®å‘ç° (Key Findings)")
        print("-" * 60)
        for finding in report.analysis.key_findings:
            print(f"\nã€{finding.category}ã€‘")
            print(f"{finding.description}")
            if finding.evidence:
                print(f"  è¯æ®:")
                for e in finding.evidence:
                    print(f"    - {e}")
            if finding.recommendations:
                print(f"  å»ºè®®:")
                for r in finding.recommendations:
                    print(f"    â€¢ {r}")
        print()

    if report.analysis.root_cause_analysis:
        print(f"\nğŸ” æ ¹å› åˆ†æ (Root Cause Analysis)")
        print("-" * 60)
        print(report.analysis.root_cause_analysis)
        print()

    if report.analysis.system_context:
        print(f"\nğŸ–¥ï¸  ç³»ç»Ÿç¯å¢ƒ (System Context)")
        print("-" * 60)
        print(
            f"```json\n{json.dumps(report.analysis.system_context, ensure_ascii=False, indent=2)}\n```"
        )
        print()

    print(f"\nğŸ“ˆ ç½®ä¿¡åº¦è¯„åˆ† (Confidence Score)")
    print("-" * 60)
    confidence = report.analysis.confidence_score
    bar_length = 20
    filled = int(bar_length * confidence)
    bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
    print(f"[{bar}] {confidence:.2%}")

    # å†™å…¥å®Œæ•´æŠ¥å‘Šåˆ°æ–‡ä»¶
    output_file = str(args.output_file)
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æŠ¥å‘Šåˆ° {output_file}...")

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"# æ—¥å¿—åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"## æ•´ä½“æ‘˜è¦\n\n{report.analysis.overall_summary}\n\n")

        if report.analysis.error_chain:
            f.write(f"## é”™è¯¯é“¾\n\n")
            f.write(f"**æ ¹æœ¬åŸå› **: {report.analysis.error_chain.root_cause}\n\n")
            f.write(f"**é”™è¯¯ä¼ æ’­é“¾**:\n\n")
            for step in report.analysis.error_chain.chain:
                f.write(f"{step.get('step')}. **{step.get('event')}** â†’ {step.get('impact')}\n")
            f.write(f"\n**æœ€ç»ˆç»“æœ**: {report.analysis.error_chain.final_outcome}\n\n")

        if report.analysis.key_findings:
            f.write(f"## å…³é”®å‘ç°\n\n")
            for finding in report.analysis.key_findings:
                f.write(f"### {finding.category}\n")
                f.write(f"{finding.description}\n\n")
                if finding.evidence:
                    f.write(f"**è¯æ®**:\n")
                    for e in finding.evidence:
                        f.write(f"- {e}\n")
                    f.write("\n")
                if finding.recommendations:
                    f.write(f"**å»ºè®®**:\n")
                    for r in finding.recommendations:
                        f.write(f"- {r}\n")
                    f.write("\n")

        if report.analysis.root_cause_analysis:
            f.write(f"## æ ¹å› åˆ†æ\n\n{report.analysis.root_cause_analysis}\n\n")

        if report.analysis.system_context:
            f.write(f"## ç³»ç»Ÿç¯å¢ƒ\n\n")
            f.write(
                f"```json\n{json.dumps(report.analysis.system_context, ensure_ascii=False, indent=2)}\n```\n\n"
            )

        f.write(f"## ç½®ä¿¡åº¦è¯„åˆ†\n\n{confidence:.2%}\n")

    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ° {output_file}")

    # æ˜¾ç¤ºå…ƒæ•°æ®
    print(f"\nğŸ“Š å¤„ç†å…ƒæ•°æ® (Processing Metadata)")
    print("-" * 60)
    print(f"  æ—¥å¿—æ¥æº: {report.metadata.log_source or 'N/A'}")
    print(f"  æ—¥å¿—å¤§å°: {report.metadata.log_size_bytes:,} å­—èŠ‚")
    print(f"  Chunkæ•°é‡: {report.metadata.chunk_count}")
    print(f"  å¤„ç†æ—¶é—´: {report.metadata.processing_time_seconds:.2f} ç§’")

    print(f"\n  ä½¿ç”¨çš„æ¨¡å‹:")
    for key, model in report.metadata.models_used.items():
        print(f"    - {key}: {model}")

    print("\n" + "=" * 60)
    print("æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
